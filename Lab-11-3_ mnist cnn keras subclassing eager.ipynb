{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab-11-3: mnist cnn keras subclassing eager.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMgOW2y7WRj2OgCrBRFW+zw"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"vCzDdpNa39YY"},"source":["import tensorflow as tf\r\n","from tensorflow import keras\r\n","from tensorflow.keras.utils import to_categorical\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7jwc1CIM3_uy"},"source":["learning_rate = 0.001\r\n","training_epochs = 15\r\n","batch_size = 100\r\n","\r\n","tf.random.set_seed(777)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zbaTCOtE3_zI"},"source":["cur_dir = os.getcwd()\r\n","ckpt_dir_name = 'checkpoints'\r\n","model_dir_name = 'minst_cnn_subclass'\r\n","\r\n","checkpoint_dir = os.path.join(cur_dir, ckpt_dir_name, model_dir_name)\r\n","os.makedirs(checkpoint_dir, exist_ok=True)\r\n","\r\n","checkpoint_prefix = os.path.join(checkpoint_dir, model_dir_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5XaUmeff3_1z"},"source":["## MNIST Dataset #########################################################\r\n","mnist = keras.datasets.mnist\r\n","class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\r\n","##########################################################################\r\n","\r\n","## Fashion MNIST Dataset #################################################\r\n","#mnist = keras.datasets.fashion_mnist\r\n","#class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\r\n","##########################################################################"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6jbWJzAM3_5T"},"source":["(train_images, train_labels), (test_images, test_labels) = mnist.load_data()    \r\n","    \r\n","train_images = train_images.astype(np.float32) / 255.\r\n","test_images = test_images.astype(np.float32) / 255.\r\n","train_images = np.expand_dims(train_images, axis=-1)\r\n","test_images = np.expand_dims(test_images, axis=-1)\r\n","    \r\n","train_labels = to_categorical(train_labels, 10)\r\n","test_labels = to_categorical(test_labels, 10)    \r\n","    \r\n","train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(\r\n","                buffer_size=100000).batch(batch_size)\r\n","test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TpW-VJ8_3_8-"},"source":["class MNISTModel(tf.keras.Model):\r\n","    def __init__(self):\r\n","        super(MNISTModel, self).__init__()\r\n","        self.conv1 = keras.layers.Conv2D(filters=32, kernel_size=[3, 3], padding='SAME', activation=tf.nn.relu)\r\n","        self.pool1 = keras.layers.MaxPool2D(padding='SAME')\r\n","        self.conv2 = keras.layers.Conv2D(filters=64, kernel_size=[3, 3], padding='SAME', activation=tf.nn.relu)\r\n","        self.pool2 = keras.layers.MaxPool2D(padding='SAME')\r\n","        self.conv3 = keras.layers.Conv2D(filters=128, kernel_size=[3, 3], padding='SAME', activation=tf.nn.relu)\r\n","        self.pool3 = keras.layers.MaxPool2D(padding='SAME')\r\n","        self.pool3_flat = keras.layers.Flatten()\r\n","        self.dense4 = keras.layers.Dense(units=256, activation=tf.nn.relu)\r\n","        self.drop4 = keras.layers.Dropout(rate=0.4)\r\n","        self.dense5 = keras.layers.Dense(units=10)\r\n","    def call(self, inputs, training=False):\r\n","        net = self.conv1(inputs)\r\n","        net = self.pool1(net)\r\n","        net = self.conv2(net)\r\n","        net = self.pool2(net)\r\n","        net = self.conv3(net)\r\n","        net = self.pool3(net)\r\n","        net = self.pool3_flat(net)\r\n","        net = self.dense4(net)\r\n","        net = self.drop4(net)\r\n","        net = self.dense5(net)\r\n","        return net"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LbWAaArO4AA_"},"source":["model = MNISTModel()\r\n","temp_inputs = keras.Input(shape=(28, 28, 1))\r\n","model(temp_inputs)\r\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SGzHXfIG4AEz"},"source":["@tf.function\r\n","def loss_fn(model, images, labels):\r\n","    logits = model(images, training=True)\r\n","    loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(\r\n","        y_pred=logits, y_true=labels, from_logits=True))     \r\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A0qUpcgI4AJC"},"source":["@tf.function\r\n","def grad(model, images, labels):\r\n","    with tf.GradientTape() as tape:\r\n","        loss = loss_fn(model, images, labels)\r\n","    return tape.gradient(loss, model.variables)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sq45tlkJ4AMV"},"source":["@tf.function\r\n","def evaluate(model, images, labels):\r\n","    logits = model(images, training=False)\r\n","    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\r\n","    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n","    return accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EEdaBSw64AQp"},"source":["optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aWawQ2hT4AUr"},"source":["checkpoint = tf.train.Checkpoint(cnn=model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VfeMh2sH4AZN"},"source":["@tf.function\r\n","def train(model, images, labels):\r\n","    grads = grad(model, images, labels)\r\n","    optimizer.apply_gradients(zip(grads, model.trainable_variables))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"itu6yzD14AdG"},"source":["\r\n","# train my model\r\n","print('Learning started. It takes sometime.')\r\n","for epoch in range(training_epochs):\r\n","    avg_loss = 0.\r\n","    avg_train_acc = 0.\r\n","    avg_test_acc = 0.\r\n","    train_step = 0\r\n","    test_step = 0\r\n","    \r\n","    for images, labels in train_dataset:\r\n","        train(model, images, labels)\r\n","        #grads = grad(model, images, labels)                \r\n","        #optimizer.apply_gradients(zip(grads, model.variables))\r\n","        loss = loss_fn(model, images, labels)\r\n","        acc = evaluate(model, images, labels)\r\n","        avg_loss = avg_loss + loss\r\n","        avg_train_acc = avg_train_acc + acc\r\n","        train_step += 1\r\n","    avg_loss = avg_loss / train_step\r\n","    avg_train_acc = avg_train_acc / train_step\r\n","    \r\n","    for images, labels in test_dataset:        \r\n","        acc = evaluate(model, images, labels)        \r\n","        avg_test_acc = avg_test_acc + acc\r\n","        test_step += 1    \r\n","    avg_test_acc = avg_test_acc / test_step    \r\n","\r\n","    print('Epoch:', '{}'.format(epoch + 1), 'loss =', '{:.8f}'.format(avg_loss), \r\n","          'train accuracy = ', '{:.4f}'.format(avg_train_acc), \r\n","          'test accuracy = ', '{:.4f}'.format(avg_test_acc))\r\n","    \r\n","    checkpoint.save(file_prefix=checkpoint_prefix)\r\n","\r\n","print('Learning Finished!')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rjxa6vww4Akm"},"source":["# train my model\r\n","print('Learning started. It takes sometime.')\r\n","for epoch in range(training_epochs):\r\n","    avg_loss = 0.\r\n","    avg_train_acc = 0.\r\n","    avg_test_acc = 0.\r\n","    train_step = 0\r\n","    test_step = 0\r\n","    \r\n","    for images, labels in train_dataset:\r\n","        train(model, images, labels)\r\n","        #grads = grad(model, images, labels)                \r\n","        #optimizer.apply_gradients(zip(grads, model.variables))\r\n","        loss = loss_fn(model, images, labels)\r\n","        acc = evaluate(model, images, labels)\r\n","        avg_loss = avg_loss + loss\r\n","        avg_train_acc = avg_train_acc + acc\r\n","        train_step += 1\r\n","    avg_loss = avg_loss / train_step\r\n","    avg_train_acc = avg_train_acc / train_step\r\n","    \r\n","    for images, labels in test_dataset:        \r\n","        acc = evaluate(model, images, labels)        \r\n","        avg_test_acc = avg_test_acc + acc\r\n","        test_step += 1    \r\n","    avg_test_acc = avg_test_acc / test_step    \r\n","\r\n","    print('Epoch:', '{}'.format(epoch + 1), 'loss =', '{:.8f}'.format(avg_loss), \r\n","          'train accuracy = ', '{:.4f}'.format(avg_train_acc), \r\n","          'test accuracy = ', '{:.4f}'.format(avg_test_acc))\r\n","    \r\n","    checkpoint.save(file_prefix=checkpoint_prefix)\r\n","\r\n","print('Learning Finished!')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"unCp-fbK4ApW"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RhsBiWX94AuH"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q1KKpE734Ayn"},"source":[""],"execution_count":null,"outputs":[]}]}