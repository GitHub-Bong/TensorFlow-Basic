{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab 06-1: Softmax classifier 를 TensorFlow 로 구현하기.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNBqDt0cUFhI7/Ti5f7K0CN"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"91zEwk2qL6jC","executionInfo":{"status":"ok","timestamp":1612444692355,"user_tz":-540,"elapsed":2413,"user":{"displayName":"유진호","photoUrl":"","userId":"08916484587507536025"}},"outputId":"70ae81ee-7c8c-440f-f53c-83bcb16ed57e"},"source":["import tensorflow as tf\r\n","import numpy as np\r\n","\r\n","x_data = [[1, 2, 1, 1],\r\n","          [2, 1, 3, 2],\r\n","          [3, 1, 3, 4],\r\n","          [4, 1, 5, 5],\r\n","          [1, 7, 5, 5],\r\n","          [1, 2, 5, 6],\r\n","          [1, 6, 6, 6],\r\n","          [1, 7, 7, 7]]\r\n","y_data = [[0, 0, 1],\r\n","          [0, 0, 1],\r\n","          [0, 0, 1],\r\n","          [0, 1, 0],\r\n","          [0, 1, 0],\r\n","          [0, 1, 0],\r\n","          [1, 0, 0],\r\n","          [1, 0, 0]]\r\n","\r\n","#convert into numpy and float format\r\n","x_data = np.asarray(x_data, dtype=np.float32)\r\n","y_data = np.asarray(y_data, dtype=np.float32)\r\n","\r\n","nb_classes = 3 #class의 개수입니다.\r\n","\r\n","print(x_data.shape)\r\n","print(y_data.shape)\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(8, 4)\n","(8, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D-Rc-VUCMWVx","executionInfo":{"status":"ok","timestamp":1612444766615,"user_tz":-540,"elapsed":702,"user":{"displayName":"유진호","photoUrl":"","userId":"08916484587507536025"}},"outputId":"d78684a9-4d82-4204-d3c9-45178aeb94e3"},"source":["#Weight and bias setting\r\n","W = tf.Variable(tf.random.normal((4, nb_classes)), name='weight')\r\n","b = tf.Variable(tf.random.normal((nb_classes,)), name='bias')\r\n","variables = [W, b]\r\n","\r\n","print(W,b)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<tf.Variable 'weight:0' shape=(4, 3) dtype=float32, numpy=\n","array([[ 0.03892944,  0.03141014,  1.2816032 ],\n","       [-0.61452633,  0.0303982 ,  0.7161511 ],\n","       [ 0.24425435,  0.39938837,  0.4733473 ],\n","       [ 0.63442904,  0.24472974, -0.4186666 ]], dtype=float32)> <tf.Variable 'bias:0' shape=(3,) dtype=float32, numpy=array([-1.9323183,  2.3536994, -0.2319128], dtype=float32)>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k9jQMCQqMo34","executionInfo":{"status":"ok","timestamp":1612444779552,"user_tz":-540,"elapsed":664,"user":{"displayName":"유진호","photoUrl":"","userId":"08916484587507536025"}},"outputId":"344a3076-c7a6-4cbd-b6be-ce15082a47fc"},"source":["# tf.nn.softmax computes softmax activations\r\n","# softmax = exp(logits) / reduce_sum(exp(logits), dim)\r\n","def hypothesis(X):\r\n","    return tf.nn.softmax(tf.matmul(X, W) + b)\r\n","\r\n","print(hypothesis(x_data))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor(\n","[[3.0545916e-03 6.3297319e-01 3.6397216e-01]\n"," [6.2163942e-03 6.1961436e-01 3.7416923e-01]\n"," [1.3935872e-02 6.3235778e-01 3.5370633e-01]\n"," [1.0976086e-02 4.5657694e-01 5.3244698e-01]\n"," [1.8310765e-04 3.7333003e-01 6.2648684e-01]\n"," [1.7404554e-02 9.5580465e-01 2.6790753e-02]\n"," [8.0399209e-04 6.8026924e-01 3.1892684e-01]\n"," [5.1685155e-04 6.5919131e-01 3.4029189e-01]], shape=(8, 3), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jMCeKl4cMsDE","executionInfo":{"status":"ok","timestamp":1612444828864,"user_tz":-540,"elapsed":675,"user":{"displayName":"유진호","photoUrl":"","userId":"08916484587507536025"}},"outputId":"4efd221c-4d36-4fa5-c74b-f05d0b9154ad"},"source":["# Softmax onehot test\r\n","sample_db = [[8,2,1,4]]\r\n","sample_db = np.asarray(sample_db, dtype=np.float32)\r\n","\r\n","print(hypothesis(sample_db))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor([[3.2904343e-05 2.0095252e-03 9.9795759e-01]], shape=(1, 3), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2a_jfZxDM158","executionInfo":{"status":"ok","timestamp":1612444847149,"user_tz":-540,"elapsed":587,"user":{"displayName":"유진호","photoUrl":"","userId":"08916484587507536025"}},"outputId":"74604f47-30ae-4604-8d20-ed063afbcb5e"},"source":["def cost_fn(X, Y):\r\n","    logits = hypothesis(X)\r\n","    cost = -tf.reduce_sum(Y * tf.math.log(logits), axis=1)\r\n","    cost_mean = tf.reduce_mean(cost)\r\n","    \r\n","    return cost_mean\r\n","\r\n","print(cost_fn(x_data, y_data))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor(2.442648, shape=(), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2CeawcHuM8kc","executionInfo":{"status":"ok","timestamp":1612444896329,"user_tz":-540,"elapsed":711,"user":{"displayName":"유진호","photoUrl":"","userId":"08916484587507536025"}},"outputId":"b43507c3-1ff6-4a70-856a-4ed6e213eaca"},"source":["x = tf.constant(3.0)\r\n","with tf.GradientTape() as g:\r\n","    g.watch(x)\r\n","    y = x * x # x^2\r\n","dy_dx = g.gradient(y, x) # Will compute to 6.0\r\n","print(dy_dx)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor(6.0, shape=(), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jQzBl-0gNIjo","executionInfo":{"status":"ok","timestamp":1612444956974,"user_tz":-540,"elapsed":586,"user":{"displayName":"유진호","photoUrl":"","userId":"08916484587507536025"}},"outputId":"b1650711-ce4d-4872-eec0-961aa5febd26"},"source":["def grad_fn(X, Y):\r\n","    with tf.GradientTape() as tape:\r\n","        loss = cost_fn(X, Y)\r\n","        grads = tape.gradient(loss, variables)\r\n","\r\n","        return grads\r\n","\r\n","print(grad_fn(x_data, y_data))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n","array([[-0.23498651,  0.28302228, -0.04803579],\n","       [-1.6147788 ,  0.7744212 ,  0.84035754],\n","       [-1.5981534 ,  0.87667525,  0.7214782 ],\n","       [-1.5950129 ,  0.87274384,  0.7222692 ]], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([-0.24336356,  0.2512647 , -0.00790114], dtype=float32)>]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TTqe9YGbNXYO","executionInfo":{"status":"ok","timestamp":1612445030812,"user_tz":-540,"elapsed":4837,"user":{"displayName":"유진호","photoUrl":"","userId":"08916484587507536025"}},"outputId":"8451934b-a1a9-4353-f78a-be57f08f431c"},"source":["def fit(X, Y, epochs=2000, verbose=100):\r\n","    optimizer =  tf.keras.optimizers.SGD(learning_rate=0.1)\r\n","\r\n","    for i in range(epochs):\r\n","        grads = grad_fn(X, Y)\r\n","        optimizer.apply_gradients(zip(grads, variables))\r\n","        if (i==0) | ((i+1)%verbose==0):\r\n","            print('Loss at epoch %d: %f' %(i+1, cost_fn(X, Y).numpy()))\r\n","            \r\n","fit(x_data, y_data)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loss at epoch 1: 1.346818\n","Loss at epoch 100: 0.652493\n","Loss at epoch 200: 0.580762\n","Loss at epoch 300: 0.523590\n","Loss at epoch 400: 0.471579\n","Loss at epoch 500: 0.422210\n","Loss at epoch 600: 0.374426\n","Loss at epoch 700: 0.328021\n","Loss at epoch 800: 0.285072\n","Loss at epoch 900: 0.258306\n","Loss at epoch 1000: 0.244498\n","Loss at epoch 1100: 0.232231\n","Loss at epoch 1200: 0.221125\n","Loss at epoch 1300: 0.211015\n","Loss at epoch 1400: 0.201766\n","Loss at epoch 1500: 0.193271\n","Loss at epoch 1600: 0.185441\n","Loss at epoch 1700: 0.178199\n","Loss at epoch 1800: 0.171482\n","Loss at epoch 1900: 0.165235\n","Loss at epoch 2000: 0.159411\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eXKIhpTaNuhg","executionInfo":{"status":"ok","timestamp":1612445063195,"user_tz":-540,"elapsed":560,"user":{"displayName":"유진호","photoUrl":"","userId":"08916484587507536025"}},"outputId":"c2213e93-ac00-4849-ea5b-1d51af4388bb"},"source":["b = hypothesis(x_data)\r\n","print(b)\r\n","print(tf.argmax(b, 1))\r\n","print(tf.argmax(y_data, 1)) # matches with y_data"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor(\n","[[9.1915107e-09 7.0366586e-05 9.9992967e-01]\n"," [5.3648133e-04 4.7331009e-02 9.5213246e-01]\n"," [4.1658952e-09 9.6879706e-02 9.0312022e-01]\n"," [1.8671566e-06 9.1204733e-01 8.7950796e-02]\n"," [1.5994181e-01 8.3323961e-01 6.8186368e-03]\n"," [8.4571652e-02 9.1542834e-01 2.2203164e-08]\n"," [8.2102346e-01 1.7897597e-01 4.5955818e-07]\n"," [9.7058916e-01 2.9410781e-02 1.2652970e-09]], shape=(8, 3), dtype=float32)\n","tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n","tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wKxR_GkoNxVh"},"source":["class softmax_classifer(tf.keras.Model):\r\n","    def __init__(self, nb_classes):\r\n","        super(softmax_classifer, self).__init__()\r\n","        self.W = tf.Variable(tf.random.normal((4, nb_classes)), name='weight')\r\n","        self.b = tf.Variable(tf.random.normal((nb_classes,)), name='bias')\r\n","        \r\n","    def softmax_regression(self, X):\r\n","        return tf.nn.softmax(tf.matmul(X, self.W) + self.b)\r\n","    \r\n","    def cost_fn(self, X, Y):\r\n","        logits = self.softmax_regression(X)\r\n","        cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.math.log(logits), axis=1))        \r\n","        return cost\r\n","    \r\n","    def grad_fn(self, X, Y):\r\n","        with tf.GradientTape() as tape:\r\n","            cost = self.cost_fn(x_data, y_data)\r\n","            grads = tape.gradient(cost, self.variables)            \r\n","            return grads\r\n","    \r\n","    def fit(self, X, Y, epochs=2000, verbose=500):\r\n","        optimizer =  tf.keras.optimizers.SGD(learning_rate=0.1)\r\n","\r\n","        for i in range(epochs):\r\n","            grads = self.grad_fn(X, Y)\r\n","            optimizer.apply_gradients(zip(grads, self.variables))\r\n","            if (i==0) | ((i+1)%verbose==0):\r\n","                print('Loss at epoch %d: %f' %(i+1, self.cost_fn(X, Y).numpy()))\r\n","            \r\n","model = softmax_classifer(nb_classes)\r\n","model.fit(x_data, y_data)"],"execution_count":null,"outputs":[]}]}