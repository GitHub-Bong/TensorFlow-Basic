{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab 10-3: Dropout.ipynb","provenance":[],"authorship_tag":"ABX9TyO4lGOG/mQT1Qyu8G+A8S5I"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"eEvEcGEsO2qq"},"source":["import tensorflow as tf\r\n","import numpy as np\r\n","from tensorflow.keras.utils import to_categorical\r\n","from tensorflow.keras.datasets import mnist\r\n","from time import time\r\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BcZusFDpQUA3"},"source":["def load(model, checkpoint_dir):\r\n","    print(\" [*] Reading checkpoints...\")\r\n","\r\n","    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\r\n","    if ckpt :\r\n","        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\r\n","        checkpoint = tf.train.Checkpoint(dnn=model)\r\n","        checkpoint.restore(save_path=os.path.join(checkpoint_dir, ckpt_name))\r\n","        counter = int(ckpt_name.split('-')[1])\r\n","        print(\" [*] Success to read {}\".format(ckpt_name))\r\n","        return True, counter\r\n","    else:\r\n","        print(\" [*] Failed to find a checkpoint\")\r\n","        return False, 0\r\n","\r\n","def check_folder(dir):\r\n","    if not os.path.exists(dir):\r\n","        os.makedirs(dir)\r\n","    return dir"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UmZ0uK8PQUDj"},"source":["def load_mnist() :\r\n","    (train_data, train_labels), (test_data, test_labels) = mnist.load_data()\r\n","    train_data = np.expand_dims(train_data, axis=-1) # [N, 28, 28] -> [N, 28, 28, 1]\r\n","    test_data = np.expand_dims(test_data, axis=-1) # [N, 28, 28] -> [N, 28, 28, 1]\r\n","\r\n","    train_data, test_data = normalize(train_data, test_data)\r\n","\r\n","    train_labels = to_categorical(train_labels, 10) # [N,] -> [N, 10]\r\n","    test_labels = to_categorical(test_labels, 10) # [N,] -> [N, 10]\r\n","\r\n","    return train_data, train_labels, test_data, test_labels\r\n","\r\n","def normalize(train_data, test_data):\r\n","    train_data = train_data.astype(np.float32) / 255.0\r\n","    test_data = test_data.astype(np.float32) / 255.0\r\n","\r\n","    return train_data, test_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5erWIdkzQUFR"},"source":["\r\n","def loss_fn(model, images, labels):\r\n","    logits = model(images, training=True)\r\n","    loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_pred=logits, y_true=labels, \r\n","                                                                   from_logits=True))\r\n","    return loss\r\n","\r\n","def accuracy_fn(model, images, labels):\r\n","    logits = model(images, training=False)\r\n","    prediction = tf.equal(tf.argmax(logits, -1), tf.argmax(labels, -1))\r\n","    accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\r\n","    return accuracy\r\n","\r\n","def grad(model, images, labels):\r\n","    with tf.GradientTape() as tape:\r\n","        loss = loss_fn(model, images, labels)\r\n","    return tape.gradient(loss, model.variables)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"86YUAMigQUH6"},"source":["def flatten() :\r\n","    return tf.keras.layers.Flatten()\r\n","\r\n","def dense(label_dim, weight_init) :\r\n","    return tf.keras.layers.Dense(units=label_dim, use_bias=True, kernel_initializer=weight_init)\r\n","\r\n","def relu() :\r\n","    return tf.keras.layers.Activation(tf.keras.activations.relu)\r\n","\r\n","def dropout(rate) :\r\n","    return tf.keras.layers.Dropout(rate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C7AcdWzBQUKb"},"source":["class create_model_class(tf.keras.Model):\r\n","    def __init__(self, label_dim):\r\n","        super(create_model_class, self).__init__()\r\n","        weight_init = tf.keras.initializers.glorot_uniform()\r\n","\r\n","        self.model = tf.keras.Sequential()\r\n","        self.model.add(flatten())\r\n","\r\n","        for i in range(4):\r\n","            self.model.add(dense(512, weight_init))\r\n","            self.model.add(relu())\r\n","            self.model.add(dropout(rate=0.5))\r\n","\r\n","        self.model.add(dense(label_dim, weight_init))\r\n","\r\n","    def call(self, x, training=None, mask=None):\r\n","\r\n","        x = self.model(x)\r\n","\r\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xBsc6uMGQUM9"},"source":["def create_model_function(label_dim) :\r\n","    weight_init = tf.keras.initializers.glorot_uniform()\r\n","\r\n","    model = tf.keras.Sequential()\r\n","    model.add(flatten())\r\n","\r\n","    for i in range(4) :\r\n","        model.add(dense(512, weight_init))\r\n","        model.add(relu())\r\n","        model.add(dropout(rate=0.5))\r\n","\r\n","    model.add(dense(label_dim, weight_init))\r\n","\r\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LyXenGVAQeqO"},"source":["\"\"\" dataset \"\"\"\r\n","train_x, train_y, test_x, test_y = load_mnist()\r\n","\r\n","\"\"\" parameters \"\"\"\r\n","learning_rate = 0.001\r\n","batch_size = 128\r\n","\r\n","training_epochs = 1\r\n","training_iterations = len(train_x) // batch_size\r\n","\r\n","label_dim = 10\r\n","\r\n","train_flag = True\r\n","\r\n","\"\"\" Graph Input using Dataset API \"\"\"\r\n","train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).\\\r\n","    shuffle(buffer_size=100000).\\\r\n","    prefetch(buffer_size=batch_size).\\\r\n","    batch(batch_size, drop_remainder=True)\r\n","\r\n","test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).\\\r\n","    shuffle(buffer_size=100000).\\\r\n","    prefetch(buffer_size=len(test_x)).\\\r\n","    batch(len(test_x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_obeMbnqQiaj"},"source":["\"\"\" Model \"\"\"\r\n","network = create_model_function(label_dim)\r\n","\r\n","\"\"\" Training \"\"\"\r\n","optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\r\n","\r\n","\"\"\" Writer \"\"\"\r\n","checkpoint_dir = 'checkpoints'\r\n","logs_dir = 'logs'\r\n","\r\n","model_dir = 'nn_dropout'\r\n","\r\n","checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\r\n","check_folder(checkpoint_dir)\r\n","checkpoint_prefix = os.path.join(checkpoint_dir, model_dir)\r\n","logs_dir = os.path.join(logs_dir, model_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wty9_2jvQicu"},"source":["if train_flag :\r\n","\r\n","    checkpoint = tf.train.Checkpoint(dnn=network)\r\n","\r\n","    # create writer for tensorboard\r\n","    summary_writer = tf.summary.create_file_writer(logdir=logs_dir)\r\n","    start_time = time()\r\n","\r\n","    # restore check-point if it exits\r\n","    could_load, checkpoint_counter = load(network, checkpoint_dir)    \r\n","\r\n","    if could_load:\r\n","        start_epoch = (int)(checkpoint_counter / training_iterations)        \r\n","        counter = checkpoint_counter        \r\n","        print(\" [*] Load SUCCESS\")\r\n","    else:\r\n","        start_epoch = 0\r\n","        start_iteration = 0\r\n","        counter = 0\r\n","        print(\" [!] Load failed...\")\r\n","    \r\n","    # train phase\r\n","    with summary_writer.as_default():  # for tensorboard\r\n","        for epoch in range(start_epoch, training_epochs):\r\n","            for idx, (train_input, train_label) in enumerate(train_dataset):            \r\n","                grads = grad(network, train_input, train_label)\r\n","                optimizer.apply_gradients(grads_and_vars=zip(grads, network.variables))\r\n","\r\n","                train_loss = loss_fn(network, train_input, train_label)\r\n","                train_accuracy = accuracy_fn(network, train_input, train_label)\r\n","                \r\n","                for test_input, test_label in test_dataset:                \r\n","                    test_accuracy = accuracy_fn(network, test_input, test_label)\r\n","\r\n","                tf.summary.scalar(name='train_loss', data=train_loss, step=counter)\r\n","                tf.summary.scalar(name='train_accuracy', data=train_accuracy, step=counter)\r\n","                tf.summary.scalar(name='test_accuracy', data=test_accuracy, step=counter)\r\n","\r\n","                print(\r\n","                    \"Epoch: [%2d] [%5d/%5d] time: %4.4f, train_loss: %.8f, train_accuracy: %.4f, test_Accuracy: %.4f\" \\\r\n","                    % (epoch, idx, training_iterations, time() - start_time, train_loss, train_accuracy,\r\n","                       test_accuracy))\r\n","                counter += 1                \r\n","        checkpoint.save(file_prefix=checkpoint_prefix + '-{}'.format(counter))\r\n","        \r\n","# test phase      \r\n","else :\r\n","    _, _ = load(network, checkpoint_dir)\r\n","    for test_input, test_label in test_dataset:    \r\n","        test_accuracy = accuracy_fn(network, test_input, test_label)\r\n","\r\n","    print(\"test_Accuracy: %.4f\" % (test_accuracy))"],"execution_count":null,"outputs":[]}]}